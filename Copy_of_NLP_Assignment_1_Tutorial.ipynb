{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dheerajkumar1a1a/Kaggle_github/blob/main/Copy_of_NLP_Assignment_1_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Packages"
      ],
      "metadata": {
        "id": "vR12D6StC-my"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can install Packages\n",
        "\n",
        "!pip install nltk\n",
        "!pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_LKfz4w2NXv",
        "outputId": "77f7654b-04bf-469a-e2ae-7590389cb995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post7.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post7-py3-none-any.whl size=2951 sha256=044075583271179a584ee048d813ead5714dbc7da9c6d9260d71fec1c824a704\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/9c/85/72901eb50bc4bc6e3b2629378d172384ea3dfd19759c77fd2c\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd && ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW6j8fA9DIsa",
        "outputId": "07ecffcc-5670-4c88-e702-76a2398e733c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "0aJyX5tQDni8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./Drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feuV5okNDR-w",
        "outputId": "93bdb160-caa6-4d10-808e-678d0125d933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at ./Drive; to attempt to forcibly remount, call drive.mount(\"./Drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK"
      ],
      "metadata": {
        "id": "ZeSY1YFnBhHV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATe88Dv20-1Q",
        "outputId": "f0011ea8-2b89-484c-c32e-65c6e797da80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Downloading Corpus from NLTK\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Download the required dataset\n",
        "nltk.download('treebank')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Treebank corpus for training\n",
        "treebank_sents = treebank.tagged_sents()\n",
        "\n",
        "# Printing a sample\n",
        "treebank_sents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgyP2TAB1ChU",
        "outputId": "2b00d7ab-b560-4a8e-e7bd-50d5ae178b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Pierre', 'NNP'),\n",
              " ('Vinken', 'NNP'),\n",
              " (',', ','),\n",
              " ('61', 'CD'),\n",
              " ('years', 'NNS'),\n",
              " ('old', 'JJ'),\n",
              " (',', ','),\n",
              " ('will', 'MD'),\n",
              " ('join', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('board', 'NN'),\n",
              " ('as', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('nonexecutive', 'JJ'),\n",
              " ('director', 'NN'),\n",
              " ('Nov.', 'NNP'),\n",
              " ('29', 'CD'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using nltk tools\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6Yh8cMFB814",
        "outputId": "c88e3d43-d76b-4301-eb49-c50ab3abb398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'My name is Abhilash Datta. I am a student of IITKGP.'\n",
        "\n",
        "tokens = sent_tokenize(sent)\n",
        "print('Sentence Tokens: ',tokens)\n",
        "print()\n",
        "\n",
        "tokens = word_tokenize(sent)\n",
        "print('Word Tokens: ',tokens)\n",
        "print()\n",
        "\n",
        "print('POS Tags: ', pos_tag(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvL52C334v5C",
        "outputId": "06b75aea-0c43-4701-e877-f7c0a6f180d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens:  ['My name is Abhilash Datta.', 'I am a student of IITKGP.']\n",
            "\n",
            "Word Tokens:  ['My', 'name', 'is', 'Abhilash', 'Datta', '.', 'I', 'am', 'a', 'student', 'of', 'IITKGP', '.']\n",
            "\n",
            "POS Tags:  [('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Abhilash', 'NNP'), ('Datta', 'NNP'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('student', 'NN'), ('of', 'IN'), ('IITKGP', 'NNP'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF32XuaICJjg",
        "outputId": "a45496f7-f129-4369-a161-47e7506d39ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'I am studying Natural Language Processing'\n",
        "words = word_tokenize(sent)\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muI_5alkCZJx",
        "outputId": "701b79a9-6ec2-4cf2-ee5c-75d5195b1d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'studi', 'natur', 'languag', 'process']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization"
      ],
      "metadata": {
        "id": "oc2pESeLC6QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "\n",
        "# Using TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
        "\n",
        "print(\"TfidfVectorizer Vocabulary:\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "print()\n",
        "\n",
        "print(\"TfidfVectorizer Vectorized Data:\")\n",
        "print()\n",
        "# print(X_tfidf.toarray())\n",
        "for i in range(len(sentences)):\n",
        "    print(sentences[i])\n",
        "    print(X_tfidf[i].toarray())\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxYFxmhT5TEd",
        "outputId": "e3bdd764-826a-4d7d-feb1-dfdda70d8913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TfidfVectorizer Vocabulary:\n",
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "\n",
            "TfidfVectorizer Vectorized Data:\n",
            "\n",
            "This is the first document.\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "\n",
            "This document is the second document.\n",
            "[[0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]]\n",
            "\n",
            "And this is the third one.\n",
            "[[0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]]\n",
            "\n",
            "Is this the first document?\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v1 = tfidf_vectorizer.transform(['second document first']).toarray()\n",
        "v2 = tfidf_vectorizer.transform(['first document second']).toarray()\n",
        "\n",
        "print('v1: ', v1)\n",
        "print('v2: ', v2)\n",
        "print()\n",
        "print(v1 == v2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV0XdBsEEaxI",
        "outputId": "8346438a-f5b0-4b38-96d0-dd6107bb0505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v1:  [[0.         0.44809973 0.55349232 0.         0.         0.70203482\n",
            "  0.         0.         0.        ]]\n",
            "v2:  [[0.         0.44809973 0.55349232 0.         0.         0.70203482\n",
            "  0.         0.         0.        ]]\n",
            "\n",
            "[[ True  True  True  True  True  True  True  True  True]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec(tokenized_sentences, vector_size=10, window=5, min_count=1, sg=0)\n",
        "\n",
        "# Get the vector representation of a word\n",
        "vector = model.wv['document']\n",
        "print(\"Vector representation of 'document':\")\n",
        "print(vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzFm2EqJ7Mnh",
        "outputId": "5db95fd9-91bd-4a3f-cb60-06ca928cafeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector representation of 'document':\n",
            "[-0.00535778  0.00236476  0.05104287  0.09009314 -0.09300297 -0.07115427\n",
            "  0.06456411  0.08971071 -0.05015172 -0.03761638]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A classical ML model"
      ],
      "metadata": {
        "id": "FOF44SapICS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the required dataset\n",
        "nltk.download('reuters')\n",
        "\n",
        "# Load the Reuters corpus\n",
        "categories = reuters.categories()\n",
        "documents = [(reuters.raw(fileid), category) for category in categories for fileid in reuters.fileids(category)]\n",
        "\n",
        "# Shuffle and split the data\n",
        "n_documents = len(documents)\n",
        "train_size = int(0.8 * n_documents)\n",
        "train_docs = documents[:train_size]\n",
        "test_docs = documents[train_size:]\n",
        "\n",
        "# Extract words and labels\n",
        "train_words = [word_tokenize(doc) for doc, _ in train_docs]\n",
        "train_labels = [(label == 'sugar') for _, label in train_docs]\n",
        "test_words = [word_tokenize(doc) for doc, _ in test_docs]\n",
        "test_labels = [(label == 'sugar') for _, label in test_docs]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgZijMFEIHgk",
        "outputId": "b6158c99-507d-4f69-ab51-b0f4c29f0172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert words to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer(max_features=200)\n",
        "X_train = vectorizer.fit_transform([' '.join(words) for words in train_words])\n",
        "X_test = vectorizer.transform([' '.join(words) for words in test_words])"
      ],
      "metadata": {
        "id": "qFJDMOO4UVqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Train a classifier (Naive Bayes)\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, train_labels)\n",
        "\n",
        "# Predict\n",
        "predictions = classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "px4RXAirUXxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test_labels, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzoBwIEGISSb",
        "outputId": "8b4364f1-f281-4016-ddf0-0f36cbb62e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.94      1.00      0.97      2504\n",
            "        True       0.00      0.00      0.00       162\n",
            "\n",
            "    accuracy                           0.94      2666\n",
            "   macro avg       0.47      0.50      0.48      2666\n",
            "weighted avg       0.88      0.94      0.91      2666\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}